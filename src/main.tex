\documentclass[sigconf,natbib=false]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[ACM REP'25]{2025 ACM Conference on Reproducibility and Replicability}{July 29-31, 2025}{Vancouver, Canada}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
\acmISBN{978-1-4503-XXXX-X/18/06}

\usepackage{tcolorbox}
\tcbuselibrary{theorems}
\usepackage{cleveref}
\newtcbtheorem[]{trap}{Pitfall}{colback=black!5,colframe=black!35,fonttitle=\bfseries}{th}
\newtcbtheorem[]{lesson}{Takeaway}{colback=black!5,colframe=black!35,fonttitle=\bfseries}{th}
\newcommand{\repro}{reproducibility}
\newcommand{\Repro}{Reproducibility}
\newcommand{\transpo}{\emph{Transposition}}
\newcommand{\flavour}{\emph{flavour}}
\newcommand{\flavours}{\emph{flavours}}
\newcommand{\ie}{\emph{i.e.,}}
\newcommand{\eg}{\emph{e.g.,}}
\newcommand{\nix}{\emph{Nix}}
\newcommand{\nixos}{\emph{NixOS}}
\newcommand{\nxc}{\emph{NixOS Compose}}
\newcommand{\enos}{\emph{EnOSlib}}
\newcommand{\grid}{\emph{Grid'5000}}
\newcommand{\kam}{\emph{Kameleon}}
\newcommand{\kad}{\emph{Kadeploy}}
\newcommand{\mel}{\emph{Melissa}}
\newcommand{\store}{\emph{Nix Store}}
\newcommand{\ad}{Artifact Description}
\newcommand{\aeval}{Artifact Evaluation}
\newcommand{\adae}{\ad/\aeval}
\newcommand{\df}{\texttt{Dockerfile}}
\newcommand{\ecg}{\texttt{ecg}}
\newcommand{\todo}[1]{{\color{red}{TODO: #1}}}
\usepackage{hyperref}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{siunitx}
%\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{calc}
%\usepackage{tabularx}
\usepackage{fontawesome}
\usepackage[para,online,flushleft]{threeparttable}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{amsmath,amsfonts}
\usepackage{textcomp}
\usepackage[inline]{enumitem}  % enhanced enumerations
\usepackage{listings}

% \newcommand{\fmc}[1]{{\color{magenta}#1}} % Florina Ciorba
\newcommand{\fmc}[1]{{}} % Florina Ciorba

\definecolor{commentcolour}{rgb}{0.04,0.43,0.17}
\definecolor{keywordcolour}{rgb}{0.65,0.15,0.64}
\definecolor{backcolour}{rgb}{1,1,1}
\definecolor{linenumbercolour}{rgb}{0.1,0.1,0.1}
\definecolor{stringcolour}{rgb}{0.56,0.06,0.49}
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},   
  commentstyle=\color{commentcolour},
  keywordstyle=\color{keywordcolour},
  numberstyle=\tiny\color{linenumbercolour},
  stringstyle=\color{stringcolour},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2,
  framexrightmargin=-12pt
}
\lstset{style=mystyle, frame=single}

%\usepackage[backend=biber,style=trad-abbrv,firstinits=true]{biblatex}
\usepackage[
  datamodel=software,
  style=trad-abbrv,
  backend=biber
]{biblatex}
\addbibresource{references.bib}
\usepackage{software-biblatex}

\begin{document}

\title{%
  SHORT: Longitudinal Study of Software Environments Produced by Dockerfiles from Research Artifacts: Initial Design% and Results
}

% \author{Quentin Guilloteau}
% \email{Quentin.Guilloteau@inria.fr}
% \affiliation{%
%   \institution{INRIA}
%   \country{France}
% }

\author{Quentin Guilloteau}
\orcid{0009-0003-7645-5044}
\email{Quentin.Guilloteau@inria.fr}
\affiliation{%
  \institution{Inria, CNRS, ENSL, UCBL1, LIP}
  \city{Lyon}
  \country{France}
}

\author{Antoine Waehren}
\orcid{0009-0003-9852-8865}
\email{A.Waehren@stud.unibas.ch}
\affiliation{%
  \institution{University of Basel}
  \city{Basel}
  \country{Switzerland}
}

\author{Florina M. Ciorba}
\orcid{0000-0002-2773-4499}
\email{Florina.Ciorba@unibas.ch}
\affiliation{%
  \institution{University of Basel}
  \city{Basel}
  \country{Switzerland}
}

% \author{Anonymous Authors}
% \email{Anonymous email}
% \affiliation{%
%   \institution{Anonymous Institution}
%   \city{Anonymous City}
%   \country{Anonymous Country}
% }


% \renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  The reproducibility crisis has affected all scientific disciplines, including computer science (CS).
  To address this issue, the CS community has established artifact evaluation processes at conferences and in journals to evaluate the reproducibility of the results shared in publications.
  Authors are therefore required to share their artifacts with reviewers, including code, data, and the software environment necessary to reproduce the results.
  One method for sharing the software environment proposed by conferences and journals is to utilize container technologies such as Docker and Apptainer.
  However, these tools rely on non-reproducible tools, resulting in non-reproducible containers.

  In this paper, we present a tool and methodology to evaluate variations over time in software environments of container images derived from research artifacts.
  We also present initial results on a small set of \df s from the Euro-Par 2024 conference.
 
\end{abstract}

%% %% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Artifact Evaluation, Containers, Longevity, Sustainability}

\received{12 February 2024}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

%% PAPER STARTS HERE -----------------------------------------------------------------------------------------------

\section{Introduction}

% \todo{split into intro and related work?}
% 
% \begin{itemize}
% \item Reproducibility crisis \cite{baker500ScientistsLift2016}
% \item Also in computer science \cite{collberg_repeatability_2015}, but most impact is from the missing environment.
% \item Artifact evaluation in conferences \cite{kidwell2016badges}
% \item Software environment is a problem/difficul aspect \cite{mytkowicz_producing_nodate}
% \item conferences recommand to use containers to share the software environmnent,
% \item more and more papers are linking code \cite{paperswithcode, kang2023papers, hong2013software}
% \item a lot of papers proposing solution around virtualisation and container \cite{brammer2011paper, brinckman2019computing}
% \item but container recipes are based on non-reproducibile tools (package managers and bad practices) \cite{guilloteau2024longevity, guilloteau2024frustrations}
% \item even though Nix \cite{dolstra_nix_2004} and Guix \cite{courtes_functional_2013} already very good solutions but not used enough \cite{guilloteau2024longevity}
% \item trade-off between storing the result of the image builts by the authors and the recipe \cite{software_heritage_2017, zenodo, figshare}
% \item we need the dev environment to \emph{inspect} and introduce \emph{variation} \cite{mercier2018considering}. \cite{blinry, nvidia_cuda_lifetime}
% \end{itemize}

The scientific community has been hit by the ``Reproducibility Crisis''~\cite{baker500ScientistsLift2016}, and computer science makes no exception~\cite{collberg_repeatability_2015}.
In order to improve the reproducibility of articles in Computer Science, the community set up, in conferences and journals, \adae\ processes~\cite{kidwell2016badges} where authors would \emph{voluntary} submit the artifact required to reproduce the main results from their article.
Then, artifact reviewers would try to reproduce those main results given the artifact from the authors.
Based on the result of the reproduction attempt, the article is awarded \emph{badges} to promote its reproducibility aspect~\cite{acm-badges}.
The link to the artifact can be found in the published version of the article, sometimes with an appendix giving more information about the artifact (\eg\ download instructions, hardware requirements, workflow)~\cite{paperswithcode, kang2023papers, hong2013software}.

One difficult aspect of creating a truly reproducible artifact is to properly control the software environment: packages and libraries versions.
Several works already shed light on the possible variability of experimental results given a badly captured software environment~\cite{mytkowicz_producing_nodate, sokolowski2024impact}.
To address this issue, the conference \adae\ committees have advised authors to provide containers or virtual machines to share their artifacts in order to ``freeze'' their software environment~\cite{eurosys25, aetips, csartifacts}.
In practice, about 12\% of the artifacts use such tools~\cite{guilloteau2024longevity}.
%However, authors generate their artifact right before the artifact submission~\cite{guilloteau2024longevity, guilloteau2024frustrations}.
In the case of usual virtualization techniques to generate containers or virtual machines, authors have to write the ``recipe'' (for example, \df\ for Docker containers) made of a list of commands to execute from a ``base image'' (the \texttt{FROM} instruction in \df s).
This approach has several limitations.
First, these base images can vary, either because of bad practices from the users (\eg\ using the \texttt{latest} version of the images) or from the administrators of the image (\eg\ pushing a different image to a previously defined tag).
Administrators can also remove these base images, making all recipes un-buildable~\cite{nvidia_cuda_lifetime}.
Another limitation of this approach is the heavy dependency on non-reproducible tools such as classical package managers (\eg\ \texttt{apt}, \texttt{dpkg}, \texttt{yum}).
Hence, rebuilding a recipe of a container or a virtual machine can yield different results from the authors' image, and thus endangers the reproducibility of the results.
One would be for the authors to store and then provide the built container itself (\ie tarball) on some long-term storage (\eg\ Zenodo~\cite{zenodo}).
But this comes with the downside of requiring more storage space than just storing the recipe (binary vs. text file), and thus has some \textbf{severe scalability and sustainability issues in terms of longevity}~\cite{monroe2023preservation, guilloteau2024longevity}.
Moreover, storing the built binary image exhibits limitations when needed to \emph{inspect} the actual content of the image, and when a modification of the software environment capture by this image is needed~\cite{mercier2018considering}.
Having the opportunity to extend previous scientific work is a crucial aspect in any scientific field.
To the best of our knowledge, no previous work has exhibited the variations in software environment build from \df s through time.

%\todo{this is already some kind of conlusion...}
%A better solution would be to use tools such as Nix~\cite{dolstra_nix_2004} or Guix~\cite{courtes_functional_2013} which are focused on reproducibility and which are able to rebuild the \emph{exact same} software environment from a textual description and with longevity properties~\cite{courtes2024source}, but these tools are unfortunately poorly adopted by the community~\cite{guilloteau2024longevity}.


In this paper, we propose our \textbf{initial design of a longitudinal study of the variations of the software environments produced by \df s from research artifacts.}
We show initial results from five \df s extracted from the Euro-Par 2024 conference reproducibility artifacts built every month for seven months, from October 1$^{st}$ 2024 to April 1$^{st}$ 2025.
We focus only on Docker containers as they are the most used in artifacts~\cite{guilloteau2024longevity}.
% \todo{The goal is not to dunk on authors or conferences, but simply to exhibit the potential issues with \df s.}

% We aim to answer the following questions:
% 
% \begin{enumerate}
% \item How does the packages evolve through time when captured in a \texttt{Dockerfile}?
% \item Which packaging tool is the most victim to variation in its resulting software environment?
% \item Which are the most volatile packages?
% \item Which practices in \df s are the most damaging to reproducibility?
% \end{enumerate}

The remainder of this paper is organized as follows.
Section~\ref{sec:ecg} presents \ecg, our software framework for this longitudinal study.
The study protocol is presented in Section~\ref{sec:protocol}.
Section~\ref{sec:results} presents the initial results of a small-scale version of the study.
Finally, we conclude and give perspectives in Section~\ref{sec:conclusion}.

% \newpage
\section{The \ecg\ Framework}\label{sec:ecg}

%% \begin{itemize}
%% \item `ecg`, Python tool, Licence, lines of code
%% \item architecture (nickel, what is supported)
%% \item process (submit new artifacts)
%% \item end goal of the project
%% \end{itemize}                   

In this section, we present \ecg, \fmc{Can we explain the acronym?} a tool for building \df s from research artifacts and collecting information on their resulting software environment.
\ecg\ is a 280-line Python3 script~\cite{ecg} under the GPLv3 license.

\subsection{Information captured by \ecg}\label{sec:ecg:capture}

This section presents the different information collected by \ecg.%, and their motivations.

\subsubsection{Cryptographic Hash of the Artifact}

The authors can share artifacts in several ways, but not all ways have the same guarantees in terms of longevity and ``stability'' of the link.
Hence, \ecg\ captures the result of the download at the given link (success or error).
In case of a successful download, \ecg\ computes the cryptographic hash of the download result to capture any variation in the source of the artifact which could later explain variations in the resulting software environment.

For example, authors can link to a Zenodo archive that will \emph{always} point to the latest artifact version.
Hence, the content of the artifact behind this link can change over time.

\subsubsection{Docker Building Error}

In the case of \df\ failing to build, \ecg\ captures the error returned by the building process (\eg\ missing image, failing command).
In fact, \ecg\ is only capable of capturing the \emph{first error} of the build even if there might be several errors in the building process.
The Docker error is extracted by \texttt{grep}ing the building process log.

\subsubsection{List of Installed Packages and their Version}

The software environment can be composed of several packages, which can have been installed by different means.
The most common way is through package managers (\eg\ \texttt{apt}, \texttt{dpkg}, \texttt{yum}).
But there are also packages installed for programming languages such as with \texttt{pip} for Python.
Another way to install the package is to download the sources and build them locally.
\ecg\ is able to capture these different ways to install packages.
Hence, for each package, \ecg\ extracts the package name, the version, and the tool used to install the package.

Currently, \ecg\ is capable of querying information from popular package managers (\texttt{dkpg} (and thus \texttt{apt}), \texttt{pacman}, \texttt{pip}, \texttt{conda}) and to deal with situations where packages are installed by source (\texttt{package\_managers} in Listing~\ref{lst:nickel}), or the case of packages installed in virtual Python environments.
In the case where there is a \texttt{git clone} command in the \df\ (\texttt{git\_packages} in Listing~\ref{lst:nickel}), \ecg\ will extract the commit of the local clone of the repository after the successful build of the container, and use this commit as a ``version''.
In the case of a download of the sources of a package (\texttt{misc\_packages} in Listing~\ref{lst:nickel}), for example with \texttt{wget} or \texttt{curl}, there is no immediate way to log the version of the package that is being downloaded.
The URL of the source code might contain a version number, but this URL might in the future point to a different version of the source code.
Hence, \ecg\ will download \emph{locally} (outside the container) the source code pointed by the URL and compute the cryptographic hash of the result to generate a ``version''.

\subsection{Artifact representation}\label{sec:nickel}

The information captured by \ecg\ cannot be automatically extracted from a \df.
In some cases, authors of artifact can have shell scripts installing packages (\eg\ calling \texttt{apt install}), which would be invisible at the \df\ level.
With \ecg, a \df\ from an artifact is represented as a Nickel configuration file~\cite{nickel}, which is similar to JSON or YAML.
%Nickel is a configuration language with a functional paradigm.
One of Nickel's features is the possibility to \emph{verify} that a configuration file is valid with respect to a \emph{contract}.
The Nickel file representing the artifact is then \emph{verified} against the contract~\cite{ecg_contract} and translated to a JSON file given as input to \ecg.
Listing~\ref{lst:nickel} shows an example of a Nickel representation.
The contract is versioned (line 2 of Listing~\ref{lst:nickel}) to take into account potential future changes to the descriptions and the contract.
If the Nickel file does not respect the contract, an error is reported to the user.

\begin{lstlisting}[caption=Example of Artifact representation in Nickel, label=lst:nickel]
{
  version = "1.0",
  artifact_url = "https://zenodo.org/.../artifact.zip",
  type = "zip",
  doi = "10.5281/zenodo.XXXXXXXX",
  conf_date = 2024,
  virtualization = "docker",
  buildfile_dir = "artifact",
  package_managers = [ "dpkg", "pip" ]
  git_packages = [
    { name = "spack", location = "/home/user/spack" }
  ],
  misc_packages = [
    {
      name = "cmake-3.22.2-linux",
      url = "https://github.com/Kitware/CMake/releases/download/v3.22.2/cmake-3.22.2-linux-x86_64.sh"
    }
  ]
}
\end{lstlisting}



\subsection{Workflow}\label{sec:workflow}

\begin{figure*}
  \centering
  \includegraphics[width=1\textwidth,trim={1.37cm 1.65cm 1.65cm 1.65cm},clip]{./figs/workflow.pdf}
  \caption{
    Workflow of \ecg.
    Each description of an artifact is verified with the Nickel contract and then converted in a JSON representation.
    This JSON representation is then read by \ecg\ to
    download the artifact,
    compute the hash of its content,
    build the container from the \df, and
    extract the software environment information from the built container.
    \ecg\ outputs files containing the information about the artifact and its \df\ .
  }
  \label{fig:workflow}
\end{figure*}

The workflow of \ecg\ is shown in Figure~\ref{fig:workflow}.
The workflow itself~\cite{ecg_workflow} is governed by the Snakemake workflow manager~\cite{koster2012snakemake}, and its software environment~\cite{ecg_nix} is managed by the Nix functional package manager~\cite{dolstra_nix_2004}.
These tools provide reproducibility and robustness to the workflow setup to ensure that we will be able to execute the exact same workflow in the exact same conditions for all the future builds of the artifacts.

% \subsection{Contributions}
% 
% This framework leaves a lot of room for contributions on the writing of the Nickel descriptions files.
% The community could contribute by submitting pull requests on the open source repository containing all the Nickel descriptions.
% \todo{more}

\section{Protocol}\label{sec:protocol}

% In this section, we present the initial protocol of our longitudinal study.
This section presents the initial protocol of our longitudinal study.

\begin{enumerate}
\item For each conference with an artifact evaluation process, we examine all artifacts and consider only those that share their software environment using a \df.
\item From the artifact and the \df\ we generate the associated Nickel file (Listing~\ref{lst:nickel}) which contains information such as the artifact's location, how to extract the \df\ from the artifact and the package managers used. 
%where is the artifact stored? how to extract the \df\ out from the artifact? which package managers are used?
\item Once all Nickel descriptions for the artifacts of a conference have been completed, we use \ecg\ to construct the download, build, and extract information from each of them.
\item Prior to building each container, the local Docker cache is cleared on these machines.
\item We will execute \ecg\ on each artifact every month, as close to the beginning of the month as possible, for a full year (\ie\ 13 months).
\end{enumerate}

\paragraph{Details}
\begin{itemize}
\item We ensure that an empty local Docker cache is used before building the containers.
\item Each container is allocated a 10-minute time frame to build.
\item \ecg\ is executed on the x86-64 machines of the \grid\ platform~\cite{grid5000} on the \texttt{dahu} cluster of the Grenoble site.
\item We do not try to reproduce the experiment results from the artifacts, but only to build the container. The impact of software environment variations on the experiments results has been studied for example in \cite{mytkowicz_producing_nodate, sokolowski2024impact}.
\end{itemize}

\section{Preliminary Results}\label{sec:results}

We wrote the descriptions in Nickel (see Section~\ref{sec:nickel}) for the five artifacts of the Euro-Par 2024 conference that used a Docker container to share their software environments, and used \ecg\ to build their associated \df s once every month for seven months.
% "2024-10-01" "2024-11-04" "2024-12-02" "2025-01-02" "2025-02-05" "2025-03-12" "2025-04-01"
The initial build was October 1$^{st}$ 2024, then the following builds were on November 4$^{th}$ 2024, December 2$^{nd}$ 2024, January 2$^{nd}$ 2025, February 5$^{th}$ 2025, March 12$^{th}$ 2025, and April 1$^{st}$ 2025.
The deadline for the authors to submit their artifacts to Euro-Par 2024 was May 15$^{th}$ 2024.
%\footnote{\url{https://2024.euro-par.org/calls/artifacts/}}.
Hence, our initial build took place almost 5 months after the artifacts submission.
The following sections follow the dimensions captured by \ecg\ (see Section~\ref{sec:ecg:capture}).
The data collected and presented in this section is available on Zenodo~\cite{dataset_europar24}.

\subsection{Hash of Artifacts}

Four out of the five articles provided a Zenodo archive to the artifact, the other provided a GitHub link with a specified \texttt{git} tag.

No variation of the artifact sources was observed, via computation of their cryptographic hash.

\subsection{Build Status}

All of the artifacts managed to successfully produce a container.

\subsection{Software Environment}

\begin{table}
  \centering
  \scalebox{0.800}{
  \begin{tabular}{llll}
    \toprule
	\multirow{2}{*}{Artifact} & \multicolumn{2}{c}{Docker Base Image used} & \multirow{2}{*}{
					\begin{tabular}{c}Calling\\ \texttt{apt update}? \end{tabular}
}\\
\cmidrule(lr){2-3}
     & Name & Version & \\
    \midrule
    \texttt{canon\_solving}~\cite{canon_solving}   & \texttt{ubuntu} & \texttt{22.04} & Yes \\
    \texttt{geijer\_how}~\cite{geijer_how}         & \texttt{ubuntu} & \texttt{22.04} & Yes \\
    \texttt{hiraga\_peanuts}~\cite{hiraga_peanuts} & \texttt{devcontainers/cpp} & \texttt{1-debian-12} & Yes \\
    \texttt{munoz\_fault}~\cite{munoz_fault}       & \texttt{ubuntu} & \texttt{22.04} & Yes \\
    \texttt{wolff\_fast}~\cite{wolff_fast}         & \texttt{ubuntu} & \texttt{22.04} & Yes \\
    \bottomrule
  \end{tabular}
  }
  \caption{Information about the \df s from the study.}\label{tab:info}
\end{table}

Table~\ref{tab:info} summarizes the information about the studied \df s.
We can see that most of the container used the \texttt{ubuntu} base image and that \emph{all} the \df s specified the version of the base image to use, thus limiting the variations in terms of the software environment produced.
However, all \df s called the \texttt{apt-get update} command, which fetches the latest versions of the packages every time and thus introduces variations.

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{./figs/results_per_artifact.pdf}
  \caption{
    Evolution of the packages in the software environment of each container over time.
    %Each container has been rebuilt once a month.
    The color of the bar corresponds to the month when a specific version of a package has been introduced in the software environment.
    We can see that the decrease of the proportion of package versions similar to the versions in the initial build over time.
  }
  \label{fig:results_per_artifact}
\end{figure*}

Figure~\ref{fig:results_per_artifact} depicts the evolution through time of the software environment produced by \df\ of each artifact studied.
We can see that even after \emph{a single month} of the initial build, the software environment is already different for \emph{all} of the artifacts studied.
This is a severe threat to reproducibility as one month is the time frame for an artifact evaluation process in conferences.
This means that the \df\ submitted by the authors can generate a different software environment between the submission of the artifact and the reproduction attempt of the reviewers.


We can see that every month new versions of the packages are being introduced in the software environments of the containers.



\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{./figs/results_per_tool.pdf}
  \caption{
    Evolution through time of the packages in the software environments managed by each tool present in the studied artifacts.
  }\label{fig:results_per_tool}
\end{figure}

Figure~\ref{fig:results_per_tool} shows the evolution of the packages' versions managed by each tool present in the studied artifact: \texttt{dpkg}, \texttt{git}, \texttt{pip}, and manual download and build (\texttt{misc}).
We can see that the tools that introduce variation in the software environment are \texttt{dpkg} and \texttt{pip}.

The packages that changed the most in terms of versions are:
\begin{enumerate}
\item \texttt{linux-libc-dev:amd64} (installed via \texttt{dpkg}) with 7 different versions for each artifact (1 per new build attempt), except for \texttt{hiraga\_peanuts} where there were 4 different versions.
\item \texttt{numpy} (installed via \texttt{pip}) with 6 different versions for the artifact \texttt{wolff\_fast}
\item \texttt{fonttools} (installed via \texttt{pip}) with 5 different versions for the artifacts \texttt{geijer\_how} and \texttt{wolff\_fast}
\end{enumerate}


% \newpage
\section{Conclusion and Perspectives}\label{sec:conclusion}

% \subsection{Conclusion}
% 
% In this paper, we presented an initial version of \ecg, and its workflow, a tool to study the variations through time of containers from research artifacts.
% We evaluated the evolution of the software environment produced by \df\ from five artifacts from the Euro-Par 2024 conference.
% We observe that, even at this small scale, after only one month, there were variations in the produced software environments.
% Out of the popular tools to manage the software environment in a container, we have seen that \texttt{dpkg} and \texttt{pip} were the ones that introducing more undesired variations.
% 
% \df s are not solutions to capture on the \emph{long term} the software environment.
% There is a trade-off between storing the Docker images built by the authors and their recipe.
% Use Nix/Guix + Software Heritage.
% 
% Even if we only looked at \df s, this study could be extended to other technologies (Apptainer, virtual machines), but it was shown in \cite{guilloteau2024longevity} that \df s was the main solution to provided virtualized software environments.
% 
% In this paper, we presented the initial design of our framework to evaluate the longevity of the software environments generated with \df s from research artifacts.
% 
% We propose three possible scenarios for the longevity of the sharing of software environment:
% 
% \paragraph{Scenario 1: ``Business as usual''}
% Authors keep submitting non-reproducible \df s and conferences keep accepting them
% 
% \paragraph{Scenario 2: Storing everything forever}
% Authors store their version of the built container onto long-term storage (\eg\ Zenodo, Figshare), and artifact reviewers do not try to rebuild the \df\ but simply download the container.
% While this solution would allow the artifact reviewers and the future researchers to use the exact same software environment as the authors, it also raises \emph{sustainability questions}.
% Indeed, with the increase of scientific publications and the increase in artifact sharing in publications, can we community continue to store the built containers which ``weight'' orders of magnitude more than the recipe to generate them?
% Services such as Zenodo and Figshare are free, and backed up by public funding.
% Can these services run out of storage? or ask for a financial participation to archive data?
% 
% 
% \paragraph{Scenario 3: Change of practice}
% In this scenario, the community, \ie\ the conferences and the authors, change their practices related to reproducibility and software environment sharing.
% The community starts adopting solutions made for reproducibility of software environment (\ie\ Nix and Guix).
% Authors then simply have to share the recipe (text files) of their software environment for the artifact reviewers and future researchers to reproduce the results of their article.
% Moreover, Nix and Guix allow for the precise introduction of variation in the software environment, which is a crucial feature (not present in container technologies) in order to build upon previous artifact for future publications~\cite{mercier2018considering}.
% 
% 
% \subsection{Perspectives}
% 
% This paper aims to be an initial version of the design of a future larger study around the longevity of containers.
% We will thus write the descriptions of more artifact from more conferences, aiming for around a hundreds total containers, from conferences of various fields of Computer Science with various ``technical'' levels.
% 
% This initial work also raised more questions:
% \begin{itemize}
% \item Does the reduction of the proportion of package versions similar to the initial build will continue decreasing or will it eventually reach a ``stable state''? (for example 50\%)
% \item ...
% \end{itemize}

% -----------------------------------------------------------------

This paper presented the initial design of our longitudinal study of the evolution of the software environments produced by \df s.
We presented in Section~\ref{sec:ecg} \ecg, and its workflow (Section~\ref{sec:workflow}), a tool to download, build, and extract information from research artifacts \df s.
Section~\ref{sec:protocol} presented the protocol to observe the potential variations in the software environment over time.
We showed initial results in Section~\ref{sec:results} on five artifacts from the Euro-Par 2024 conference built every month for seven months.

While this initial study only considered five \df s, it revealed that \textbf{the software environments produced by the considered \df s vary \emph{every month}} which highlights the lack of reproducibility of such tools and methods to share software environments.
The question of the results' significance on such a small sample will need to be answered in a larger-scale study.

\textbf{
  A practice to deal with the lack of reproducibility of \df s is that the authors store the built container on long-term storage such as Zenodo.
  However, this practice raises serious questions in terms of scalability and sustainability in terms of storage~\cite{monroe2023preservation}.
  The lack of reproducibility of \df s should not be hidden behind long-term storage: the practices of the authors and of \adae\ need to be re-evaluated.
}

The community may want to adopt instead solutions \emph{tailored for long-term reproducibility} of software environment~\cite{malka2024reproducibility} (\ie\ Nix~\cite{dolstra_nix_2004} and Guix~\cite{courtes_functional_2013}), where authors can share a textual representation of their software environment which could be precisely reproduced and with longevity guarantees~\cite{courtes2024source}.
Moreover, Nix and Guix allow the precise introduction of variation in the software environment, which is a crucial feature (not present in container technologies) in order to build upon previous artifact for future publications~\cite{mercier2018considering}.

\paragraph{Future Perspectives}

%This paper aims to be an initial version of the design of a future larger study on the longevity of containers.
% We will consider not only Docker containers, but also other containerization tools such as Apptainer.
% We will thus write the descriptions of more artifacts from more conferences, aiming for around a hundreds total containers, from conferences of various fields of Science with various Computer Science technical levels.
This paper serves as the foundation for the design of a larger future study on the longevity of containers. 
We will consider not only Docker containers, but also other containerization tools such as Apptainer.
Therefore, we will write descriptions of more artifacts from more conferences, aiming at a total of approximately a hundred containers, from conferences spanning various scientific disciplines and with varying levels of computer science technical expertise.

%
% /////////////////////////////////////////////////////////////////////////////////////////////////////

\newpage
\section*{Acknowledgments}

%Antoine?

Experiments presented in this paper were carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RENATER and several Universities as well as other organizations (see https://www.grid5000.fr). 

This research was funded in part by the European Unionâ€™s Horizon 2020 research and innovation programme under grant agreement No. 957407 (DAPHNE).

%% PAPER ENDS HERE -----------------------------------------------------------------------------------------------

% \newpage
% \newpage
%\bibliographystyle{sty/ACM-Reference-Format}
%\bibliography{references}
\printbibliography

\end{document}
